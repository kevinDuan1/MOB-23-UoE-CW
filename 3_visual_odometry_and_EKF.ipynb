{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3042bc3f",
   "metadata": {},
   "source": [
    "## (Stereo) Visual Odometry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eed9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319f1fca",
   "metadata": {},
   "source": [
    "### Dataset Handling\n",
    "Let's make a dataset handling object to make our data more accessible as we complete our task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f3c7c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_Handler():\n",
    "    def __init__(self, lidar=False, progress_bar=True):\n",
    "        import pandas as pd\n",
    "        import os\n",
    "        import cv2\n",
    "\n",
    "        self.left_image_files = os.listdir('./data/image_02/data')\n",
    "        self.right_image_files = os.listdir('./data/image_03/data')\n",
    "        self.left_image_files.sort()\n",
    "        self.right_image_files.sort()\n",
    "\n",
    "        self.num_frames = len(self.left_image_files)\n",
    "\n",
    "        self.reset_frames()\n",
    "            # Store original frame to memory for testing functions\n",
    "        self.first_image_left = cv2.imread('./data/image_02/data/' \n",
    "                                               + self.left_image_files[0], 0)\n",
    "        \n",
    "        self.first_image_right = cv2.imread('./data/image_03/data/' \n",
    "                                               + self.right_image_files[0], 0)\n",
    "        self.second_image_left = cv2.imread('./data/image_02/data/' \n",
    "                                               + self.left_image_files[1], 0)\n",
    "        self.imheight = self.first_image_left.shape[0]\n",
    "        self.imwidth = self.first_image_left.shape[1]\n",
    "            \n",
    "            \n",
    "    def reset_frames(self):\n",
    "        # Resets all generators to the first frame of the sequence\n",
    "        self.images_left = (cv2.imread('./data/image_02/data/'+ name_left, 0)\n",
    "                            for name_left in self.left_image_files)\n",
    "        self.images_right = (cv2.imread('./data/image_03/data/' + name_right, 0)\n",
    "                            for name_right in self.right_image_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a6e19f",
   "metadata": {},
   "source": [
    "#### disparity map and depth map\n",
    "Unlike the simulation dataset we used in the tutorial, this time we are going to use a real driving dataset. Therefore we don't have depth images that you need to use for PnP directly, however, we have stereo images in this dataset, so the depth map can be generated from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25863ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_left_disparity_map(img_left, img_right, matcher='bm', rgb=False, verbose=False):\n",
    "    '''\n",
    "    Takes a left and right stereo pair of images and computes the disparity map for the left\n",
    "    image. Pass rgb=True if the images are RGB.\n",
    "    \n",
    "    Arguments:\n",
    "    img_left -- image from left camera\n",
    "    img_right -- image from right camera\n",
    "    \n",
    "    Optional Arguments:\n",
    "    matcher -- (str) can be 'bm' for StereoBM or 'sgbm' for StereoSGBM matching\n",
    "    rgb -- (bool) set to True if passing RGB images as input\n",
    "    verbose -- (bool) set to True to report matching type and time to compute\n",
    "    \n",
    "    Returns:\n",
    "    disp_left -- disparity map for the left camera image\n",
    "    \n",
    "    '''\n",
    "    # Feel free to read OpenCV documentation and tweak these values. These work well\n",
    "    sad_window = 6\n",
    "    num_disparities = sad_window*16\n",
    "    block_size = 11\n",
    "    matcher_name = matcher\n",
    "    \n",
    "    if matcher_name == 'bm':\n",
    "        matcher = cv2.StereoBM_create(numDisparities=num_disparities,\n",
    "                                      blockSize=block_size\n",
    "                                     )\n",
    "        \n",
    "    elif matcher_name == 'sgbm':\n",
    "        matcher = cv2.StereoSGBM_create(numDisparities=num_disparities,\n",
    "                                        minDisparity=0,\n",
    "                                        blockSize=block_size,\n",
    "                                        P1 = 8 * 3 * sad_window ** 2,\n",
    "                                        P2 = 32 * 3 * sad_window ** 2,\n",
    "                                        mode=cv2.STEREO_SGBM_MODE_SGBM_3WAY\n",
    "                                       )\n",
    "    if rgb:\n",
    "        img_left = cv2.cvtColor(img_left, cv2.COLOR_BGR2GRAY)\n",
    "        img_right = cv2.cvtColor(img_right, cv2.COLOR_BGR2GRAY)\n",
    "    start = datetime.datetime.now()\n",
    "    disp_left = matcher.compute(img_left, img_right).astype(np.float32)/16\n",
    "    end = datetime.datetime.now()\n",
    "    if verbose:\n",
    "        print(f'Time to compute disparity map using Stereo{matcher_name.upper()}:', end-start)\n",
    "    \n",
    "    return disp_left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9e25ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = Dataset_Handler()\n",
    "left = handler.first_image_left\n",
    "right = handler.first_image_right\n",
    "disp = compute_left_disparity_map(left, \n",
    "                                  right, \n",
    "                                  matcher='sgbm',\n",
    "                                  verbose=True)\n",
    "plt.figure(figsize=(11,7))\n",
    "plt.imshow(disp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c005076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_projection_matrix(p):\n",
    "    '''\n",
    "    Shortcut to use cv2.decomposeProjectionMatrix(), which only returns k, r, t, and divides\n",
    "    t by the scale, then returns it as a vector with shape (3,) (non-homogeneous)\n",
    "    \n",
    "    Arguments:\n",
    "    p -- projection matrix to be decomposed\n",
    "    \n",
    "    Returns:\n",
    "    k, r, t -- intrinsic matrix, rotation matrix, and 3D translation vector\n",
    "    \n",
    "    '''\n",
    "    k, r, t, _, _, _, _ = cv2.decomposeProjectionMatrix(p)\n",
    "    t = (t / t[3])[:3]\n",
    "    \n",
    "    return k, r, t\n",
    "\n",
    "def calc_depth_map(disp_left, k_left, t_left, t_right, rectified=True):\n",
    "    '''\n",
    "    Calculate depth map using a disparity map, intrinsic camera matrix, and translation vectors\n",
    "    from camera extrinsic matrices (to calculate baseline). Note that default behavior is for\n",
    "    rectified projection matrix for right camera. If using a regular projection matrix, pass\n",
    "    rectified=False to avoid issues.\n",
    "    \n",
    "    Arguments:\n",
    "    disp_left -- disparity map of left camera\n",
    "    k_left -- intrinsic matrix for left camera\n",
    "    t_left -- translation vector for left camera\n",
    "    t_right -- translation vector for right camera\n",
    "    \n",
    "    Optional Arguments:\n",
    "    rectified -- (bool) set to False if t_right is not from rectified projection matrix\n",
    "    \n",
    "    Returns:\n",
    "    depth_map -- calculated depth map for left camera\n",
    "    \n",
    "    '''\n",
    "    # Get focal length of x axis for left camera\n",
    "    f = k_left[0][0]\n",
    "    \n",
    "    # Calculate baseline of stereo pair\n",
    "    if rectified:\n",
    "        b = t_right[0] - t_left[0] \n",
    "    else:\n",
    "        b = t_left[0] - t_right[0]\n",
    "        \n",
    "    # Avoid instability and division by zero\n",
    "    disp_left[disp_left == 0.0] = 0.1\n",
    "    disp_left[disp_left == -1.0] = 0.1\n",
    "    \n",
    "    # Make empty depth map then fill with depth\n",
    "    depth_map = np.ones(disp_left.shape)\n",
    "    depth_map = f * b / disp_left\n",
    "    \n",
    "    return depth_map\n",
    "def stereo_2_depth(img_left, img_right, P0, P1, matcher='bm', rgb=False, verbose=False, \n",
    "                   rectified=True):\n",
    "    '''\n",
    "    Takes stereo pair of images and returns a depth map for the left camera. If your projection\n",
    "    matrices are not rectified, set rectified=False.\n",
    "    \n",
    "    Arguments:\n",
    "    img_left -- image of left camera\n",
    "    img_right -- image of right camera\n",
    "    P0 -- Projection matrix for the left camera\n",
    "    P1 -- Projection matrix for the right camera\n",
    "    \n",
    "    Optional Arguments:\n",
    "    matcher -- (str) can be 'bm' for StereoBM or 'sgbm' for StereoSGBM\n",
    "    rgb -- (bool) set to True if images passed are RGB. Default is False\n",
    "    verbose -- (bool) set to True to report computation time and method\n",
    "    rectified -- (bool) set to False if P1 not rectified to P0. Default is True\n",
    "    \n",
    "    Returns:\n",
    "    depth -- depth map for left camera\n",
    "    \n",
    "    '''\n",
    "    # Compute disparity map\n",
    "    disp = compute_left_disparity_map(img_left, \n",
    "                                      img_right, \n",
    "                                      matcher=matcher, \n",
    "                                      rgb=rgb, \n",
    "                                      verbose=verbose)\n",
    "    # Decompose projection matrices\n",
    "    k_left, r_left, t_left = decompose_projection_matrix(P0)\n",
    "    k_right, r_right, t_right = decompose_projection_matrix(P1)\n",
    "    # Calculate depth map for left camera\n",
    "    depth = calc_depth_map(disp, k_left, t_left, t_right)\n",
    "    \n",
    "    return depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f476faf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = np.array([[7.188560e+02 ,   0.    ,  6.071928e+02,   4.538225e+01],\n",
    "               [  0.    ,7.188560e+02 , 1.852157e+02,   -1.130887e-01],\n",
    "               [  0.    ,   0.    ,   1.    ,   3.779761e-03]])\n",
    "P1 = np.array([[7.188560e+02 ,   0.    ,  6.071928e+02,   -3.372877e+02],\n",
    "               [  0.    ,7.188560e+02 , 1.852157e+02,   2.369057e+00],\n",
    "               [  0.    ,   0.    ,   1.    ,   4.915215e-03]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b5e6fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = stereo_2_depth(left, \n",
    "                       right, \n",
    "                       P0, \n",
    "                       P1,\n",
    "                       matcher='sgbm',\n",
    "                       verbose=True)\n",
    "plt.imshow(depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0881e8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can constuct a mask using this information like so\n",
    "mask = np.zeros(handler.first_image_left.shape[:2], dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83333b4f",
   "metadata": {},
   "source": [
    "### Your task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dc694c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(image, detector='orb', mask=None):\n",
    "    \"\"\"\n",
    "    Find keypoints and descriptors for the image\n",
    "\n",
    "    Arguments:\n",
    "    image -- a grayscale image\n",
    "    mask -- Masks for each input image specifying where to look for keypoints\n",
    "\n",
    "    Returns:\n",
    "    kp -- list of the extracted keypoints (features) in an image\n",
    "    des -- list of the keypoint descriptors in an image\n",
    "    \"\"\"\n",
    "    pass\n",
    "    \n",
    "    return kp, des"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e627e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_features(des1, des2, matching='BF', detector='sift', k=2):\n",
    "    \"\"\"\n",
    "    Match features from two images\n",
    "    You can set nfeatures to 500 for a faster computation\n",
    "\n",
    "    Arguments:\n",
    "    des1 -- list of the keypoint descriptors in the first image\n",
    "    des2 -- list of the keypoint descriptors in the second image\n",
    "    matching -- (str) can be 'BF' for Brute Force or 'FLANN'\n",
    "    detector -- (str) can be 'sift or 'orb'. Default is 'sift'\n",
    "    k -- (int) number of neighbors to match to each feature.\n",
    "\n",
    "    Returns:\n",
    "    matches -- list of matched features from two images. Each match[i] is k or less matches for \n",
    "               the same query descriptor\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925f96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_matches_distance(matches, dist_threshold):\n",
    "    \"\"\"\n",
    "    Filter matched features from two images by distance between the best matches\n",
    "\n",
    "    Arguments:\n",
    "    match -- list of matched features from two images\n",
    "    dist_threshold -- maximum allowed relative distance between the best matches, (0.0, 1.0) \n",
    "\n",
    "    Returns:\n",
    "    filtered_match -- list of good matches, satisfying the distance threshold\n",
    "    \"\"\"\n",
    "    pass\n",
    "    return filtered_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b2a3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matches(image1, kp1, image2, kp2, match):\n",
    "    \"\"\"\n",
    "    Visualize corresponding matches in two images\n",
    "\n",
    "    Arguments:\n",
    "    image1 -- the first image in a matched image pair\n",
    "    kp1 -- list of the keypoints in the first image\n",
    "    image2 -- the second image in a matched image pair\n",
    "    kp2 -- list of the keypoints in the second image\n",
    "    match -- list of matched features from the pair of images\n",
    "\n",
    "    Returns:\n",
    "    image_matches -- an image showing the corresponding matches on both image1 and image2 or None if you don't use this function\n",
    "    \"\"\"\n",
    "    image_matches = cv2.drawMatches(image1, kp1, image2, kp2, match, None, flags=2)\n",
    "    plt.figure(figsize=(16, 6), dpi=100)\n",
    "    plt.imshow(image_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077b67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_motion(match, kp1, kp2, k, depth1=None, max_depth=3000):\n",
    "    \"\"\"\n",
    "    Estimate camera motion from a pair of subsequent image frames\n",
    "\n",
    "    Arguments:\n",
    "    match -- list of matched features from the pair of images\n",
    "    kp1 -- list of the keypoints in the first image\n",
    "    kp2 -- list of the keypoints in the second image\n",
    "    k -- camera intrinsic calibration matrix \n",
    "    \n",
    "    Optional arguments:\n",
    "    depth1 -- Depth map of the first frame. Set to None to use Essential Matrix decomposition\n",
    "    max_depth -- Threshold of depth to ignore matched features. 3000 is default\n",
    "\n",
    "    Returns:\n",
    "    rmat -- estimated 3x3 rotation matrix\n",
    "    tvec -- estimated 3x1 translation vector\n",
    "    image1_points -- matched feature pixel coordinates in the first image. \n",
    "                     image1_points[i] = [u, v] -> pixel coordinates of i-th match\n",
    "    image2_points -- matched feature pixel coordinates in the second image. \n",
    "                     image2_points[i] = [u, v] -> pixel coordinates of i-th match\n",
    "               \n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "    return rmat, tvec, image1_points, image2_points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290b567b",
   "metadata": {},
   "source": [
    "### check your implementation here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b2d36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_left = handler.first_image_left\n",
    "plt.imshow(image_left)\n",
    "image_right = handler.first_image_right \n",
    "image_plus1 = handler.second_image_left \n",
    "depth = stereo_2_depth(image_left, image_right, P0,  P1, matcher='sgbm', verbose=True) \n",
    "kp0, des0 = extract_features(image_left, 'orb') \n",
    "kp1, des1 = extract_features(image_plus1, 'orb') \n",
    "matches = match_features(des0, des1, matching='BF', detector='orb') \n",
    "print('Number of matches before filtering:', len(matches)) \n",
    "matches = filter_matches_distance(matches, 0.8) \n",
    "print('Number of matches after filtering:', len(matches)) \n",
    "visualize_matches(image_left, kp0, image_plus1, kp1, matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40d5c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visual_odometry(handler, detector='sift', matching='BF', filter_match_distance=None, \n",
    "                    stereo_matcher='sgbm', mask=None, depth_type='stereo'):\n",
    "    '''\n",
    "    Function to perform visual odometry on a sequence from the KITTI visual odometry dataset.\n",
    "    Takes as input a Data_Handler object and optional parameters.\n",
    "    \n",
    "    Arguments:\n",
    "    handler -- Data_Handler object instance\n",
    "    \n",
    "    Optional Arguments:\n",
    "    detector -- (str) can be 'sift' or 'orb'. Default is 'sift'.\n",
    "    matching -- (str) can be 'BF' for Brute Force or 'FLANN'. Default is 'BF'.\n",
    "    filter_match_distance -- (float) value for ratio test on matched features. Default is None.\n",
    "    stereo_matcher -- (str) can be 'bm' (faster) or 'sgbm' (more accurate). Default is 'bm'.\n",
    "    mask -- (array) mask to reduce feature search area to where depth information available.\n",
    "    depth_type -- (str) can be 'stereo' or set to None to use Essential matrix decomposition.\n",
    "                        Note that scale will be incorrect with no depth used.\n",
    "   \n",
    "    Returns:\n",
    "    trajectory -- Array of shape Nx3x4 of estimated poses of vehicle for each computed frame.\n",
    "    \n",
    "    '''\n",
    "    P0 = np.array([[7.188560e+02 ,   0.    ,  6.071928e+02,   4.538225e+01],\n",
    "               [  0.    ,7.188560e+02 , 1.852157e+02,   -1.130887e-01],\n",
    "               [  0.    ,   0.    ,   1.    ,   3.779761e-03]])\n",
    "    P1 = np.array([[7.188560e+02 ,   0.    ,  6.071928e+02,   -3.372877e+02],\n",
    "               [  0.    ,7.188560e+02 , 1.852157e+02,   2.369057e+00],\n",
    "               [  0.    ,   0.    ,   1.    ,   4.915215e-03]])\n",
    "    # Report methods being used to user\n",
    "    print('Generating disparities with Stereo{}'.format(str.upper(stereo_matcher)))\n",
    "    print('Detecting features with {} and matching with {}'.format(str.upper(detector), \n",
    "                                                                   matching))\n",
    "    if filter_match_distance is not None:\n",
    "        print('Filtering feature matches at threshold of {}*distance'.format(filter_match_distance))\n",
    "    \n",
    "    pass\n",
    "\n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3919d6cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(handler.first_image_left.shape[:2], dtype=np.uint8)\n",
    "ymax = handler.first_image_left.shape[0]\n",
    "xmax = handler.first_image_left.shape[1]\n",
    "cv2.rectangle(mask, (96,0), (xmax,ymax), (255), thickness = -1)\n",
    "plt.imshow(mask)\n",
    "# You can set nfeatures to 500 and use FLANN based matcher for a faster computation\n",
    "trajectory = visual_odometry(handler,\n",
    "                                        filter_match_distance=0.5, \n",
    "                                        detector='sift', # you can use your favourite detector and matcher\n",
    "                                        matching='BF',\n",
    "                                        stereo_matcher='bm',\n",
    "                                        mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f241527",
   "metadata": {},
   "source": [
    "#### Load  gps data\n",
    "Given that GPS have a noise following a normal distribution of 0 mean and 0.5 standard deviation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa92ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_data = np.load('data/gps.npy')\n",
    "gps_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8226e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect('equal')\n",
    "ax.plot(trajectory[:, :, 3][:, 0], \n",
    "        trajectory[:, :, 3][:, 2], \n",
    "        label='estimated', color='orange')\n",
    "\n",
    "ax.scatter(gps_data[:,0],gps_data[:,1],\n",
    "        label='gps',color='red')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee41914",
   "metadata": {},
   "source": [
    "##### You might need this in following part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c547c541",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_pose = np.zeros((handler.num_frames-1,2))\n",
    "bearing = np.zeros(handler.num_frames)\n",
    "relative_angle = np.zeros(handler.num_frames-1)\n",
    "distance = np.zeros(handler.num_frames-1)\n",
    "for i in range(1,handler.num_frames):\n",
    "    delta_pose[i-1,0] = trajectory[:, :, 3][i,0]-trajectory[:, :, 3][i-1,0]\n",
    "    delta_pose[i-1,1] = trajectory[:, :, 3][i,2]-trajectory[:, :, 3][i-1,2]\n",
    "    bearing[i] = np.arctan2(delta_pose[i-1,0],delta_pose[i-1,1])\n",
    "    relative_angle[i-1] = bearing[i] - bearing[i-1]\n",
    "    distance[i-1] = np.sqrt(np.square(delta_pose[i-1,0]) + np.square(delta_pose[i-1,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5eda26",
   "metadata": {},
   "source": [
    "## Extended Kalman Filter\n",
    "\n",
    "Now please try to use EKF to fuse the visual odometry from cameras with gps.\n",
    "\n",
    "For simplicity, set the state vector X = \\begin{bmatrix}\n",
    " b \\\\\n",
    " x \\\\\n",
    " y\n",
    "\\end{bmatrix}\n",
    "### Motion Model\n",
    "\\begin{align}\n",
    "\\mathbf{x}_{k} &= \n",
    "\\begin{bmatrix}\n",
    "1 &0 & 0 \\\\\n",
    "0 &1 & 0\\\\\n",
    "0 &0 & 1\n",
    "\\end{bmatrix}\n",
    "\\mathbf{x}_{k-1} +\n",
    "\\begin{bmatrix}\n",
    "1 &0 \\\\\n",
    "0 &sin(b)\\\\\n",
    "0 &cos(b)\n",
    "\\end{bmatrix}\n",
    "\\left(\n",
    "\\begin{bmatrix}\n",
    "\\theta \\\\\n",
    "d\n",
    "\\end{bmatrix}\n",
    "+ \\mathbf{w}_k\n",
    "\\right)\n",
    "\\, , \\, \\, \\, \\, \\, \\mathbf{w}_k = \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{Q}\\right)\n",
    "\\end{align}\n",
    "\n",
    "- $\\mathbf{x}_k = \\left[ b \\, x \\, y  \\right]^T$ is the current bearing and 2d position of the vehicle\n",
    "- $\\theta $ is the change in bearing between frame k-1 and k, data is stored in \"relative_angle\"\n",
    "- $d$ is the distance traveled between frame k-1 and k, data is stored in \"distance\"\n",
    "\n",
    "The process noise $\\mathbf{w}_k$ has a (zero mean) normal distribution with a constant covariance $\\mathbf{Q}$.\n",
    "\n",
    "\n",
    "### Measurement Model\n",
    "\n",
    "The measurement model from gps $\\mathbf{y}_k = \\left[x \\, y \\right]^T$.\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{y}_k =\n",
    "\\begin{bmatrix}\n",
    "0 & 1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "x_k\n",
    "+ \\mathbf{n}_k\n",
    "\\, , \\, \\, \\, \\, \\, \\mathbf{n}_k = \\mathcal{N}\\left(\\mathbf{0}, \\mathbf{R}\\right)\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "The gps measurement noise $\\mathbf{n}_k$ has a (zero mean) normal distribution with a constant covariance $\\mathbf{R}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8132c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import inv\n",
    "init_x = np.array([0,0,0])\n",
    "x_check_hist = [init_x]\n",
    "P_init = np.diag([0, 0, 0])\n",
    "P_check_hist = [P_init]\n",
    "th_var = 0.01\n",
    "d_var = 0.01\n",
    "Q_km = np.diag([th_var, d_var])\n",
    "cov_y = np.diag([1, 1])\n",
    "\n",
    "for i in range(handler.num_frames-1):\n",
    "    b = x_check_hist[-1][0]\n",
    "    d = distance[i]\n",
    "    x_check = np.zeros((3,))\n",
    "    x_check[0] = b + relative_angle[i]\n",
    "    x_check[1] = x_check_hist[-1][1] + d*np.sin(b)\n",
    "    x_check[2] = x_check_hist[-1][2] + d*np.cos(b)\n",
    "    \n",
    "\n",
    "    F_km = np.mat([[1,0,0],\n",
    "                   [d*np.cos(b),1,0],\n",
    "                   [-d*np.sin(b),0,1]])\n",
    "    L_km = np.mat([[1, 0],\n",
    "                   [0, np.sin(b)],\n",
    "                   [0, np.cos(b)]])\n",
    "\n",
    "    P_check = F_km.dot(P_check_hist[-1]).dot(F_km.T) + L_km.dot(Q_km).dot(L_km.T)\n",
    "    \n",
    "    H_k = np.matrix([[0, 1, 0],\n",
    "                  [0, 0, 1]])\n",
    "    M_k = np.eye(2)\n",
    "    K_k = P_check.dot(H_k.T).dot(inv(H_k.dot(P_check).dot(H_k.T) + M_k.dot(cov_y).dot(M_k.T)))\n",
    "    \n",
    "    y_k_l_predict = np.zeros([2, 1])\n",
    "    y_k_l_predict[0] = x_check_hist[-1][1] + d*np.sin(b)\n",
    "    y_k_l_predict[1] = x_check_hist[-1][2] + d*np.cos(b)\n",
    "    y_k_l = np.mat([[gps_data[i,0]], [gps_data[i,1]]])\n",
    "    add = K_k.dot(y_k_l - y_k_l_predict)\n",
    "    add = np.array(add).flatten()\n",
    "    x_check = x_check + add\n",
    "    P_check = (np.eye(3) - K_k.dot(H_k)).dot(P_check)\n",
    "    x_check_hist.append(x_check)\n",
    "    P_check_hist.append(P_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f772b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set_aspect('equal')\n",
    "ax.plot(trajectory[:, :, 3][:, 0], \n",
    "        trajectory[:, :, 3][:, 2], \n",
    "        label='estimated', color='orange')\n",
    "ax.scatter(gps_data[:,0],gps_data[:,1],\n",
    "        label='gps',color='red',s=1)\n",
    "ax.plot(...,label='EKF')#plot your EKF result here\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6757c820",
   "metadata": {},
   "source": [
    "In a similar vein, now consider a real-life fusion problem for IMU and WIFI localization. Unlike outdoor localization tasks, indoor localization tasks lack reliable GPS results and therefore often use technologies such as bluetooth, WiFi, etc. to aid localization. You are given the Informatics Forum first floor dense IMU odometer results and sparse WiFi positioning results, and use EKF to fuse them. Optionally you can incorporate the uncertainty from the WiFi localization result to the filter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5418e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imu = pd.read_csv(\"./data/WiFi_IMU/pdr.csv\")\n",
    "imu = imu[['tmsp','x','y']].to_numpy() #time, odometery frame x, odometery frame y\n",
    "wifi = pd.read_csv(\"./data/WiFi_IMU/wifi.csv\")\n",
    "wifi = wifi[['time','x','y','uncertainty_normalized']].to_numpy() #time, map frame x, map frame y, uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065fbe7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### your implementation:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370033d4",
   "metadata": {},
   "source": [
    "Plot your result on the floor plan:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769e1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def meter2pixel(x,y,fig_resolution=72,fig_scale=100):\n",
    "    pix_x=x/0.0254/fig_scale*fig_resolution\n",
    "    pix_y=-y/0.0254/fig_scale*fig_resolution\n",
    "    return pix_x,pix_y\n",
    "\n",
    "floorplan = plt.imread(\"./data/WiFi_IMU/F1.png\")\n",
    "gt = np.load(\"./data/WiFi_IMU/gt_9.npy\")\n",
    "gtx_pixel,gty_pixel= meter2pixel(gt[:,1],gt[:,2])\n",
    "\n",
    "plt.imshow(floorplan)\n",
    "plt.plot(gtx_pixel,gty_pixel,label=\"GT\")\n",
    "plt.legend()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
