{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e09857-57bf-4dcf-9968-fd439fdd8ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8f33fa-48d2-4ae7-b9ed-6c3208deda1e",
   "metadata": {},
   "source": [
    "# DICE evaluation metric\n",
    "In the lab semantic segmentation, you have implemented IOU to evaluate the performance of the model. Here, you need to implement a similar evaluation metric called DICE or Sørensen–Dice coefficient, and it is formulated as: $$ DICE(X, X_{truth}) = \\frac{2|X \\cap X_{truth}|}{|X| + |X_{truth}|}$$ \\\n",
    "Compared to IOU, DICE is more sensitive to small differences in overlap due to the squared terms in the numerator and denominator, so it can be more informative when there's a need to discriminate between segmentations with subtle differences in overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba67de8-cad4-4620-afc8-40f897b4d8e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def DICE(inp : Tensor, tgt : Tensor):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "        inp: Predicted mask (batchsize, number of classes, width, height)\n",
    "        tgt: Ground truth mask (batchsize, number of classes, width, height)\n",
    "    Returns:\n",
    "        Classwise Average of DICE coefficient\n",
    "    \"\"\"\n",
    "    eps = 1e-5 # small number to add to denominator to avoid division by zero\n",
    "    #YOUR CODE START HERE\n",
    "    sum_dim = (-1, -2, -3)\n",
    "    # calculation of intersection   \n",
    "    inter = 2 *(inp * tgt).sum(dim=sum_dim)\n",
    "\n",
    "    # calculate the sum of |inp| + |tgt|\n",
    "    sets_sum = inp.sum(dim=sum_dim) + tgt.sum(dim=sum_dim)\n",
    "    sets_sum = torch.where(sets_sum == 0, inter, sets_sum)\n",
    "\n",
    "    # calcaute the dice    \n",
    "    dice = (inter + eps) / (sets_sum + eps)\n",
    "    \n",
    "    # average the dice batchwise\n",
    "    return dice.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e549bf-b9d9-4937-a364-d2252338df8e",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee88cf73-b96f-425b-bda1-c8d693b2ee97",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction1 = Tensor([[0, 7, 5, 7, 2],\n",
    "        [2, 4, 5, 9, 9],\n",
    "        [2, 8, 5, 1, 8],\n",
    "        [3, 6, 5, 2, 6],\n",
    "        [3, 2, 9, 1, 1]]).unsqueeze(0).long()\n",
    "mask1 = Tensor([[4, 2, 5, 0, 2],\n",
    "        [8, 2, 9, 8, 5],\n",
    "        [0, 8, 7, 9, 6],\n",
    "        [8, 6, 5, 9, 1],\n",
    "        [3, 2, 9, 0, 6]]).unsqueeze(0).long()\n",
    "prediction2 = Tensor([[5, 7, 3, 3, 0],\n",
    "        [0, 2, 8, 2, 7],\n",
    "        [1, 7, 0, 9, 9],\n",
    "        [7, 5, 2, 3, 4],\n",
    "        [6, 0, 9, 0, 1]]).unsqueeze(0).long()\n",
    "mask2 = Tensor([[4, 6, 8, 3, 0],\n",
    "        [4, 4, 7, 2, 7],\n",
    "        [0, 0, 4, 9, 9],\n",
    "        [5, 2, 3, 3, 4],\n",
    "        [3, 0, 0, 8, 2]]).unsqueeze(0).long()\n",
    "\n",
    "#Tests\n",
    "dice1 = DICE(F.one_hot(prediction1).permute(0, 3, 1, 2).float(), F.one_hot(mask1).permute(0, 3, 1, 2).float()).item()\n",
    "dice2 = DICE(F.one_hot(prediction2).permute(0, 3, 1, 2).float(), F.one_hot(mask2).permute(0, 3, 1, 2).float()).item()\n",
    "\n",
    "assert np.isclose(0.3200001120567322, dice1), 'incorrect dice 1!'\n",
    "assert np.isclose(0.3600001037120819, dice2), 'incorrect dice 2!'\n",
    "print(\"\\033[92m All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90afc2aa-d450-4057-8fc0-52e2fd400e65",
   "metadata": {},
   "source": [
    "Based on their formulation, what are the **difference** among Cross-entropy, DICE, and IOU?  \n",
    "\n",
    "Your answer starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2deb627-e1d3-4c9c-8397-8d1db7e0b6c0",
   "metadata": {},
   "source": [
    "# Evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b49fe-633c-4abd-8736-b26dda22df11",
   "metadata": {},
   "source": [
    "Now report the Cross-entropy, DICE, and IOU for a model of your preference. You can use a pretrained model from the lab or train new model, but keep in mind that your model is able to take in inputs of size $(B, 3, 128, 128)$ and output prediction of $(B, C, 128, 128)$, where $B$ is batch size and $C$ is number of classes (8 in our case). The preprocessed image and masks from the lab are loaded as Torch.tensor below:\\\n",
    "**Marks will be awarded for both correctness and creativity**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692f00fc-b2cf-48c7-98b1-e52a9be0a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regard these preprocessed image and masks as your test set\n",
    "input_images = torch.load('input_images.pth')\n",
    "masks = torch.load('masks.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096b6678-eb2c-4e7e-9cc3-fbcb46b96acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#YOUR CODE START HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
